*****Running model = BertForMaskedLM.from_pretrained(args.loadmodel)*****
The Initial Date = 6-15
bert is training which based on corpus ./train_data/duie_ner_train_1_spo_noWork_text.txt
The log information is saved in : ./log/bert/duie_ner_train_1_spo_noWork_text_20220615101635_rbtl1_train_log.txt
epoch = 0


*****For ee == 0, gg == 0:*****
*****inputs: {'input_ids': tensor([[  101,  2548,   103,  5389,   103,  6444,   103,  1905,  8381,   103,
           818, 14275,   103,   103,  4638,   818, 14275,  3221,  6444, 16446,
          3297,  4542, 20467,  3297,  3475, 15854,  4638,   103, 16485,  8024,
          3118, 15955,   103, 15583,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0],
        [  101,  6818, 16246,  8024,  3342, 17130,   712, 15955,  4638,   517,
          5636, 14241,   103, 18499,   518,  3064, 14196,  8024,  5869, 16266,
         17460,   868, 13768,  1649, 15218,  4633, 13734,  5688, 17737,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0],
        [  101,  2110,   103,  2768, 15275,   862, 15825, 16885,  8024,  1298,
         13833, 15417, 18802, 14977, 15167,  5739, 19484,  3315, 17963,  1469,
          4798, 14951,   103,  2918, 15071,  1952, 16229, 20412,   103, 15167,
          1528, 15167, 14357,   103,  8024,  1762,  3211,   103, 17552,  4777,
         18012,   510,  6659,   103, 14322,  2767, 14253,  4777, 18012,   680,
          2821, 19454,  4415, 19446,  3175, 20538,  1357, 15590,   808, 13839,
         17801, 17737,  4638,  2768,   103,  8024,   712, 15955,  1744, 15214,
          4852, 17963,  1825, 20089, 20612, 17737,   103,   103,  8024,   103,
         17730,  6598, 14278,  4638,   103, 17834,   103, 20636,   753, 20612,
          8024,  1298, 13833,   103, 15167,   100, 11263,   100,  1825, 14822,
           103, 14278,  6440, 20636,   671, 20612,   102,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0],
        [  101,   124,   119,   517,  1158, 14817,  4638,  1920,  1921, 13943,
           518,  4507, 16834,  3481, 16690, 16837,  2857, 13875,   103, 16485,
           680,  4664, 17776,   103, 13562,   103, 14817,  4638,  1920,   103,
         13943,   518,  5143, 14211,  3173, 13925,   517,  1158, 14817, 12311,
          8798,   518,  8024,  3221,   809, 14241,   103,  3125, 13809,  6814,
           103,   122,   674,   123,  1283,   103,  1400,  4638,  3313, 16398,
           686, 17575,   868, 13768,  5659, 14435,  8024,  1059, 16230,  4633,
         14824,  4638,  6235, 18739,   103, 13890,  3022, 13790,  3173, 14855,
          4638,  3322, 16519,  1921, 13943,  8024,   680,  3341, 18689,  2460,
         16670, 14096,  4638,  4742, 13839,   812,   868, 15830,  8024,  3315,
           103,   704,   103, 14435,  2792, 14819,  4638,  6125, 19944,  3221,
           809,  3313, 16398,  4638,  7676, 17006,   711,  3519, 15630,  6822,
         19178,  6392, 19426,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1]])}*****
*****np.shape(inputs): (3,)*****
*****np.shape(inputs['input_ids']): torch.Size([4, 124])*****
*****np.shape(inputs['token_type_ids']): torch.Size([4, 124])*****
*****np.shape(inputs['attention_mask']): torch.Size([4, 124])*****
*****labels: tensor([[ -100,  -100, 14871,  -100, 16485,  -100, 16446,  -100,   712, 19263,
          -100,  -100,  9767,  8169,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  7028,  -100,  -100,
          -100,  -100,  3791,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100],
        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  6121,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100],
        [ -100,  -100, 16375,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  8024,  -100,  -100,  -100,  -100,  -100, 14977,  -100,
          -100,  -100,  -100, 14951,  -100,  -100,  -100, 14358,  -100,  -100,
          -100,  -100,  -100, 16209,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100, 15275,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,   671, 20612,  -100,  3616,
          -100,  -100,  -100,  -100,  4906,  -100, 19497,  -100,  -100,  -100,
          -100,  -100,  -100, 14977,  -100,  -100,  -100,  -100,  -100,  -100,
          6598,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100],
        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1333,  -100,
           680,  -100, 17776,  4638,   517,  1158,  -100,  -100,  -100,  1921,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,   868,  -100,  -100,  -100,
           749,  -100,  -100,  -100,  -100, 15456,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 16230,  -100,
          -100,  -100,  -100,  -100,  2199,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  3127,  -100,  -100,  -100,  -100,  -100,  -100,
         13925,  -100,  5659,  -100,  -100,  -100,  -100,  6125,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100]])*****
*****np.shape(labels): torch.Size([4, 124])*****

*****in forward() of BertForMaskedLM*****
*****outputs: BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.6170, -0.1671,  0.2122,  ...,  0.0321, -0.1485, -1.1844],
         [-0.2137, -0.1426, -0.1469,  ...,  0.1554, -1.0739, -0.1875],
         [ 0.7866,  0.7403, -0.2916,  ...,  0.4789, -2.0542, -1.0898],
         ...,
         [ 0.2063,  0.6793, -1.3787,  ...,  0.3185, -0.0048, -1.2779],
         [ 0.5274,  0.6791, -1.1860,  ...,  0.3592, -0.1654, -1.3770],
         [ 0.4820,  0.2526, -0.8264,  ...,  0.4320, -0.5008, -1.2362]],

        [[ 0.1691,  0.1371,  0.1197,  ..., -0.2553, -0.2061, -0.2018],
         [-0.7904, -0.5544, -1.5481,  ..., -1.0659,  0.8467,  0.0868],
         [-0.5823, -0.1087, -1.3586,  ...,  0.6556, -0.1840,  0.0101],
         ...,
         [ 0.3538,  1.5027, -0.3919,  ..., -0.1129, -0.6915, -0.3010],
         [ 0.4010,  1.3014, -0.2767,  ..., -0.2808, -0.4521, -0.2379],
         [ 0.2733,  1.4003, -0.6031,  ..., -0.0098, -0.6009, -0.2953]],

        [[ 0.2005,  0.3503, -0.0452,  ..., -0.2661, -0.1900, -0.8296],
         [-0.1473,  0.2276,  0.7623,  ..., -1.5274,  0.0601, -0.3388],
         [ 0.6857,  0.9287, -1.1303,  ...,  0.4973, -0.2552, -1.1096],
         ...,
         [ 0.3171,  0.6048, -1.8815,  ...,  0.6442, -0.2026, -1.8659],
         [ 0.4366,  0.4806, -1.6625,  ...,  0.8115, -0.3357, -1.7507],
         [ 0.4792,  0.6507, -1.9191,  ...,  0.7559, -0.3469, -1.6774]],

        [[ 0.1517, -0.2418, -0.5484,  ...,  0.0603,  0.2280, -0.4050],
         [ 1.6162,  0.6571, -0.3078,  ..., -0.4734, -0.0770,  0.3539],
         [ 1.1546, -0.4035, -1.2365,  ..., -0.1981,  0.0105,  0.0724],
         ...,
         [ 0.1854,  0.0986,  0.3318,  ...,  0.0796,  1.1136, -0.5345],
         [ 0.0554,  0.1032, -1.2258,  ...,  1.0378,  0.4245, -0.3786],
         [ 0.1520, -0.2416, -0.5485,  ...,  0.0605,  0.2280, -0.4050]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward>), pooler_output=None, hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)*****
*****np.shape(outputs): ()*****
*****sequence_output: tensor([[[ 0.6170, -0.1671,  0.2122,  ...,  0.0321, -0.1485, -1.1844],
         [-0.2137, -0.1426, -0.1469,  ...,  0.1554, -1.0739, -0.1875],
         [ 0.7866,  0.7403, -0.2916,  ...,  0.4789, -2.0542, -1.0898],
         ...,
         [ 0.2063,  0.6793, -1.3787,  ...,  0.3185, -0.0048, -1.2779],
         [ 0.5274,  0.6791, -1.1860,  ...,  0.3592, -0.1654, -1.3770],
         [ 0.4820,  0.2526, -0.8264,  ...,  0.4320, -0.5008, -1.2362]],

        [[ 0.1691,  0.1371,  0.1197,  ..., -0.2553, -0.2061, -0.2018],
         [-0.7904, -0.5544, -1.5481,  ..., -1.0659,  0.8467,  0.0868],
         [-0.5823, -0.1087, -1.3586,  ...,  0.6556, -0.1840,  0.0101],
         ...,
         [ 0.3538,  1.5027, -0.3919,  ..., -0.1129, -0.6915, -0.3010],
         [ 0.4010,  1.3014, -0.2767,  ..., -0.2808, -0.4521, -0.2379],
         [ 0.2733,  1.4003, -0.6031,  ..., -0.0098, -0.6009, -0.2953]],

        [[ 0.2005,  0.3503, -0.0452,  ..., -0.2661, -0.1900, -0.8296],
         [-0.1473,  0.2276,  0.7623,  ..., -1.5274,  0.0601, -0.3388],
         [ 0.6857,  0.9287, -1.1303,  ...,  0.4973, -0.2552, -1.1096],
         ...,
         [ 0.3171,  0.6048, -1.8815,  ...,  0.6442, -0.2026, -1.8659],
         [ 0.4366,  0.4806, -1.6625,  ...,  0.8115, -0.3357, -1.7507],
         [ 0.4792,  0.6507, -1.9191,  ...,  0.7559, -0.3469, -1.6774]],

        [[ 0.1517, -0.2418, -0.5484,  ...,  0.0603,  0.2280, -0.4050],
         [ 1.6162,  0.6571, -0.3078,  ..., -0.4734, -0.0770,  0.3539],
         [ 1.1546, -0.4035, -1.2365,  ..., -0.1981,  0.0105,  0.0724],
         ...,
         [ 0.1854,  0.0986,  0.3318,  ...,  0.0796,  1.1136, -0.5345],
         [ 0.0554,  0.1032, -1.2258,  ...,  1.0378,  0.4245, -0.3786],
         [ 0.1520, -0.2416, -0.5485,  ...,  0.0605,  0.2280, -0.4050]]],
       device='cuda:0', grad_fn=<NativeLayerNormBackward>)*****
*****np.shape(sequence_output): torch.Size([4, 124, 1024])*****
*****prediction_scores: tensor([[[ 2.6884,  3.0734,  3.1467,  ...,  0.2375,  3.3228,  1.8960],
         [-0.8571, -0.7134,  1.2077,  ..., -0.6692,  1.5856,  0.6975],
         [ 1.0559,  1.2829,  2.7291,  ...,  0.4123,  2.0587,  1.2971],
         ...,
         [ 0.9198,  0.2157,  0.2468,  ..., -0.2147,  2.0991,  0.9853],
         [ 0.9262, -0.0738, -0.0822,  ..., -0.2234,  1.8439,  0.5520],
         [ 0.5870, -0.1658,  0.1277,  ..., -0.3509,  1.7423,  0.5810]],

        [[ 3.1212,  2.6146,  2.5187,  ...,  1.0601,  3.1788,  1.9412],
         [ 0.9561,  0.5524,  0.6425,  ...,  1.3031,  2.3217,  1.8673],
         [ 0.9186,  1.6457,  2.2074,  ...,  1.4122,  2.4168,  2.8759],
         ...,
         [-0.8299, -1.4060, -0.2706,  ..., -0.7439, -0.0652, -0.1027],
         [-0.0467, -0.1660,  0.2709,  ...,  0.2610,  0.7182,  0.2780],
         [-1.1972, -1.6153, -0.4814,  ..., -1.0836, -0.4107, -0.2191]],

        [[ 3.3072,  3.5282,  3.4715,  ...,  0.7700,  3.9704,  2.1665],
         [ 0.6742,  0.2511,  2.3278,  ..., -0.3887,  1.4514,  1.7158],
         [ 2.0276,  3.4475,  4.1689,  ...,  0.5032,  4.1376,  3.2525],
         ...,
         [-0.5770, -1.1009, -1.7568,  ..., -2.6873,  0.7196, -1.7540],
         [ 0.2891, -0.7776, -1.1604,  ..., -2.4892,  0.9745, -0.6805],
         [-0.1306, -1.0060, -0.9059,  ..., -2.7286,  0.8553, -0.9928]],

        [[ 3.6280,  3.4622,  3.2581,  ...,  0.8736,  3.2773,  2.5255],
         [-1.2228,  1.7069,  0.1845,  ..., -1.5612, -0.6017, -0.4292],
         [-0.9057, -0.2876, -0.1326,  ..., -0.7516,  0.1510,  0.6899],
         ...,
         [ 1.1572,  2.4382,  1.8419,  ...,  1.0362,  2.1559,  1.9078],
         [-0.0318,  0.6994, -0.2083,  ..., -0.2847, -0.2401,  1.2775],
         [ 3.6282,  3.4623,  3.2582,  ...,  0.8740,  3.2771,  2.5256]]],
       device='cuda:0', grad_fn=<AddBackward0>)*****
*****np.shape(prediction_scores): torch.Size([4, 124, 21128])*****
*****masked_lm_loss: 10.911073684692383*****

*****For ee == 0, gg == 0:*****
*****outputs.logits: tensor([[[ 2.6884,  3.0734,  3.1467,  ...,  0.2375,  3.3228,  1.8960],
         [-0.8571, -0.7134,  1.2077,  ..., -0.6692,  1.5856,  0.6975],
         [ 1.0559,  1.2829,  2.7291,  ...,  0.4123,  2.0587,  1.2971],
         ...,
         [ 0.9198,  0.2157,  0.2468,  ..., -0.2147,  2.0991,  0.9853],
         [ 0.9262, -0.0738, -0.0822,  ..., -0.2234,  1.8439,  0.5520],
         [ 0.5870, -0.1658,  0.1277,  ..., -0.3509,  1.7423,  0.5810]],

        [[ 3.1212,  2.6146,  2.5187,  ...,  1.0601,  3.1788,  1.9412],
         [ 0.9561,  0.5524,  0.6425,  ...,  1.3031,  2.3217,  1.8673],
         [ 0.9186,  1.6457,  2.2074,  ...,  1.4122,  2.4168,  2.8759],
         ...,
         [-0.8299, -1.4060, -0.2706,  ..., -0.7439, -0.0652, -0.1027],
         [-0.0467, -0.1660,  0.2709,  ...,  0.2610,  0.7182,  0.2780],
         [-1.1972, -1.6153, -0.4814,  ..., -1.0836, -0.4107, -0.2191]],

        [[ 3.3072,  3.5282,  3.4715,  ...,  0.7700,  3.9704,  2.1665],
         [ 0.6742,  0.2511,  2.3278,  ..., -0.3887,  1.4514,  1.7158],
         [ 2.0276,  3.4475,  4.1689,  ...,  0.5032,  4.1376,  3.2525],
         ...,
         [-0.5770, -1.1009, -1.7568,  ..., -2.6873,  0.7196, -1.7540],
         [ 0.2891, -0.7776, -1.1604,  ..., -2.4892,  0.9745, -0.6805],
         [-0.1306, -1.0060, -0.9059,  ..., -2.7286,  0.8553, -0.9928]],

        [[ 3.6280,  3.4622,  3.2581,  ...,  0.8736,  3.2773,  2.5255],
         [-1.2228,  1.7069,  0.1845,  ..., -1.5612, -0.6017, -0.4292],
         [-0.9057, -0.2876, -0.1326,  ..., -0.7516,  0.1510,  0.6899],
         ...,
         [ 1.1572,  2.4382,  1.8419,  ...,  1.0362,  2.1559,  1.9078],
         [-0.0318,  0.6994, -0.2083,  ..., -0.2847, -0.2401,  1.2775],
         [ 3.6282,  3.4623,  3.2582,  ...,  0.8740,  3.2771,  2.5256]]],
       device='cuda:0', grad_fn=<AddBackward0>)*****
*****np.shape(outputs.logits): torch.Size([4, 124, 21128])*****
*****outputs.loss: 10.911073684692383*****
*****np.shape(outputs.loss): torch.Size([])*****
*****masked_label: tensor([14871, 16485, 16446,   712, 19263,  9767,  8169,  7028,  3791,  6121,
        16375,  8024, 14977, 14951, 14358, 16209, 15275,   671, 20612,  3616,
         4906, 19497, 14977,  6598,  1333,   680, 17776,  4638,   517,  1158,
         1921,   868,   749, 15456, 16230,  2199,  3127, 13925,  5659,  6125],
       device='cuda:0')*****
*****np.shape(masked_label): torch.Size([40])*****
*****masked_pre: tensor([  395,  7487,  6642, 19894,  1694,  1055, 10466, 11892,  2871,  6894,
         7487,  7487,  7935,  7487,  8638,  6094, 10593,  6197,  7487, 11636,
         6094,  5490,  6160,  8400,  1096,   380,  4964,  7487, 10038,  6683,
         7906,  6866,  4347,  5354,  7863, 10733, 11834,  1488,  8934,  1075],
       device='cuda:0')*****
*****np.shape(masked_pre): torch.Size([40])*****
