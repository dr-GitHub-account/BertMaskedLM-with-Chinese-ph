*****Running model = BertForMaskedLM.from_pretrained(args.loadmodel)*****
The Initial Date = 6-14
bert is training which based on corpus ./train_data/duie_ner_train_1_spo_noWork_text.txt
The log information is saved in : ./log/bert/duie_ner_train_1_spo_noWork_text_20220614212958_rbtl1_train_log.txt
epoch = 0
*****For ee == 0, gg == 0:*****
*****inputs: {'input_ids': tensor([[  101,  2528, 16293,  ...,     0,     0,     0],
        [  101,  7716, 17963,  ...,     0,     0,     0],
        [  101,  2768, 20020,  ...,  2692, 13778,   102],
        ...,
        [  101,  1744, 15214,  ...,     0,     0,     0],
        [  101,  7481, 15247,  ...,     0,     0,     0],
        [  101,   103,   185,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]])}*****
*****np.shape(inputs): (3,)*****
*****np.shape(inputs['input_ids']): torch.Size([16, 92])*****
*****np.shape(inputs['token_type_ids']): torch.Size([16, 92])*****
*****np.shape(inputs['attention_mask']): torch.Size([16, 92])*****
*****labels: tensor([[ -100,  -100, 16293,  ...,  -100,  -100,  -100],
        [ -100,  -100, 17963,  ...,  -100,  -100,  -100],
        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],
        ...,
        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],
        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],
        [ -100,  2129,  -100,  ...,  -100,  -100,  -100]])*****
*****np.shape(labels): torch.Size([16, 92])*****
*****For ee == 0, gg == 0:*****
*****outputs.logits: tensor([[[ 2.4958e+00,  3.3257e+00,  3.5031e+00,  ...,  3.6294e-01,
           3.6450e+00,  1.9962e+00],
         [-5.3739e-01,  5.6270e-01,  2.1764e+00,  ..., -7.8946e-02,
           1.2610e+00,  1.6407e+00],
         [-2.2874e-01,  6.8956e-01,  2.0288e+00,  ..., -1.3817e+00,
           7.9149e-01, -4.0110e-01],
         ...,
         [ 8.5670e-01,  8.7433e-01,  1.7550e+00,  ..., -4.4227e-01,
           2.6685e+00,  1.2526e+00],
         [ 8.0096e-01,  7.5070e-01,  1.7182e+00,  ..., -3.3816e-01,
           2.2893e+00,  1.1581e+00],
         [ 7.6733e-01,  6.7391e-01,  1.6851e+00,  ..., -2.7095e-01,
           2.1699e+00,  1.4547e+00]],

        [[ 1.8173e+00,  2.8937e+00,  3.1501e+00,  ...,  4.5200e-01,
           3.3012e+00,  1.2954e+00],
         [ 4.7278e-01,  1.8490e+00,  2.8784e+00,  ...,  9.4139e-01,
           3.0154e+00,  2.1711e+00],
         [ 1.0767e+00,  1.2489e+00,  2.7868e+00,  ..., -5.1700e-01,
           2.2962e+00,  5.7379e-01],
         ...,
         [-4.8901e-02, -8.1239e-01,  3.1423e-01,  ..., -2.0649e+00,
           1.1867e+00, -9.1196e-01],
         [ 1.6087e-02, -7.6658e-01,  4.9368e-01,  ..., -1.7752e+00,
           1.2236e+00, -7.0621e-01],
         [ 3.3780e-01, -1.9079e-01,  5.2607e-01,  ..., -1.1738e+00,
           1.4428e+00,  2.0399e-01]],

        [[ 2.9495e+00,  3.1713e+00,  2.9392e+00,  ...,  6.7235e-01,
           3.7430e+00,  2.2731e+00],
         [ 1.8011e+00,  1.7894e+00,  2.4750e+00,  ...,  1.4771e+00,
           3.2191e+00,  2.7504e+00],
         [ 7.3654e-01, -3.3782e-01,  1.4248e+00,  ..., -8.3989e-01,
           3.5227e-01,  8.7729e-01],
         ...,
         [-8.9374e-01, -2.7119e-01,  1.9268e-01,  ..., -1.5999e-01,
          -9.8019e-02,  7.5342e-01],
         [-1.2764e+00, -7.0353e-01, -2.4816e+00,  ..., -1.7253e+00,
          -4.9837e-01,  8.8166e-02],
         [ 2.9502e+00,  3.1726e+00,  2.9403e+00,  ...,  6.7337e-01,
           3.7445e+00,  2.2739e+00]],

        ...,

        [[ 1.1600e+00,  1.5434e+00,  9.9925e-01,  ..., -9.3084e-01,
           1.8947e+00,  4.1707e-02],
         [-8.2760e-01, -4.2673e-01,  6.2835e-01,  ..., -8.0381e-01,
           1.9598e+00,  8.7013e-02],
         [-4.0665e-01, -4.4004e-01,  1.4477e+00,  ..., -9.3996e-01,
           1.8542e+00, -1.4331e+00],
         ...,
         [-2.7211e+00, -1.9618e+00, -1.1168e+00,  ..., -2.8147e+00,
           1.8483e-01, -2.0650e+00],
         [-1.8981e+00, -1.2627e+00, -9.2875e-01,  ..., -2.6567e+00,
           6.5478e-01, -1.8196e+00],
         [-1.7344e+00, -1.1325e+00,  2.4594e-01,  ..., -1.9232e+00,
           9.0087e-01,  3.5399e-01]],

        [[ 2.1334e+00,  2.7541e+00,  2.8657e+00,  ..., -3.4211e-02,
           2.7158e+00,  1.9458e+00],
         [-2.3537e+00, -1.7956e+00,  6.0411e-01,  ..., -1.9717e+00,
          -1.3823e+00,  1.9205e-01],
         [-3.1114e+00, -2.5909e+00, -2.1259e+00,  ..., -2.4995e+00,
          -1.7065e+00, -3.0613e+00],
         ...,
         [-6.9391e-01, -1.3859e+00,  1.9924e-01,  ..., -2.8523e+00,
          -4.1718e-02, -2.8694e-01],
         [-6.4655e-01, -1.1263e+00, -1.3010e-02,  ..., -2.6280e+00,
          -4.5281e-02, -3.9863e-02],
         [-1.2109e+00, -1.6040e+00, -9.7772e-01,  ..., -2.7546e+00,
           1.7560e-01, -1.4204e-01]],

        [[ 8.7438e-01,  1.1573e+00,  1.5093e+00,  ..., -3.5234e-01,
           1.4473e+00,  5.0705e-02],
         [ 1.8163e+00,  2.7913e+00,  4.5458e+00,  ...,  2.1668e+00,
           2.2336e+00,  2.3675e+00],
         [ 1.6921e-01, -6.6281e-02,  1.7575e+00,  ..., -1.1033e+00,
           7.0370e-01, -1.1254e-01],
         ...,
         [-2.4143e-01,  2.1010e-03,  1.0110e+00,  ..., -6.1057e-01,
           1.5231e+00,  5.0030e-01],
         [-2.1637e-01,  2.3609e-01,  1.1235e+00,  ..., -6.0489e-01,
           1.5835e+00,  6.8796e-01],
         [-1.8674e-01,  3.6031e-01,  1.2975e+00,  ..., -4.4466e-01,
           1.5305e+00,  6.6753e-01]]], device='cuda:0', grad_fn=<AddBackward0>)*****
*****np.shape(outputs.logits): torch.Size([16, 92, 21128])*****
*****outputs.loss: 10.617749214172363*****
*****np.shape(outputs.loss): torch.Size([])*****
*****masked_label: tensor([16293,  8024, 15167,  4852, 15167, 15167, 18200, 17963, 14910,  6817,
         4413,   855, 18447,  3221,  2679,  2141,  1217,  5445, 17851, 17962,
        20623,  1372, 19294,  2798,  1762,  4638,  4510,  6820, 19507,   783,
        14706, 18046, 15050, 17963, 13943,  2797, 17716,  6371,  1297, 17442,
        14096, 17717,  7573, 14456, 19141, 16894,  5408,  3221,  3221, 14977,
         8024, 20087,  3668,  5680,  6381, 13985,  1957,  4638,  1957,  6632,
        15210,   518, 19489, 13912,  2768, 17282,   679, 16755, 18739,  8024,
        15455,  1139,   671, 13748, 19262,  8024, 12458,  5101, 14009, 15429,
         4989, 15165, 15456, 16919, 16241,  7362,  7942,  8715, 14977,   704,
        15167, 15167, 18109, 17472, 15167, 15674, 19866, 10075, 14270,   754,
         8024,   711, 14824,  3299,  8024,  6981,  3727, 21006, 19450,  6851,
        20084,   517,  4638,  2548,  5052, 16411,  5811,  1466, 14523, 19930,
         3797,  4374, 19871, 18046,  4638, 19436,  6810,  2129,   517, 20496,
         7390, 14962, 16510], device='cuda:0')*****
*****np.shape(masked_label): torch.Size([133])*****
*****masked_pre: tensor([ 6322,  7487,  2312,  1460,  7487,  6745,  7487,  1370,  1370,  6988,
         7723,  6866,  1460,  1460,  4037,  8885,  2483,  7959,  1553,  7440,
        11310,  1629,  8297, 12179,   832, 13162,  5605,  7664,  7664,  3178,
         1236,  9325,  6815,  7664,   825,  7505,  7242, 10767,  3646, 11291,
         4228,  2654,  2988,  7487,  7935,  7487,  3451,  6197,  9041,  2777,
         6662,  4347,  9041,  6435,  7958,   522,   531,  7882,  7487,  4009,
         5766,  1552,  6093,  3440,  6617,  6453,  4725,  1055,  2988, 13526,
         6970, 11302,  4979,   952,  1493,  4611,  4852,  2640,  5497,  3335,
         7487,  6584,  3646,  4548,  1415,  6053,  6439,  3359,  3099,  6197,
         7487,  1873,  6006,  7487,  3178,  1622,  4319,  4833,  6103, 18418,
         5586,  4560,  2330,  6197,  7487,  7487,  7723,  3921,   827,  7722,
         2312, 11963,   825,   784,  2871,  7487, 11636,  4801,  8465,  5021,
         3076,  6796,  2970,  6642,  1514,  6139, 11834, 11864, 11025,  4924,
         4924,  7487,  6507], device='cuda:0')*****
*****np.shape(masked_pre): torch.Size([133])*****
	 batch = 400 	 loss = 4.44064 	 acc = 0.305 	 cost_time = 87.170s
	 batch = 800 	 loss = 3.61038 	 acc = 0.339 	 cost_time = 173.431s
	 batch = 1200 	 loss = 3.02515 	 acc = 0.527 	 cost_time = 259.636s
	 batch = 1600 	 loss = 2.54541 	 acc = 0.567 	 cost_time = 345.885s
	 batch = 2000 	 loss = 2.48403 	 acc = 0.565 	 cost_time = 431.882s
	 batch = 2400 	 loss = 2.25231 	 acc = 0.598 	 cost_time = 518.184s
	 batch = 2800 	 loss = 2.15432 	 acc = 0.623 	 cost_time = 604.619s
	 batch = 3200 	 loss = 2.00308 	 acc = 0.684 	 cost_time = 690.845s
	 batch = 3600 	 loss = 2.04537 	 acc = 0.593 	 cost_time = 777.396s
	 batch = 4000 	 loss = 1.92230 	 acc = 0.678 	 cost_time = 864.138s
	 batch = 4400 	 loss = 2.48366 	 acc = 0.524 	 cost_time = 950.607s
	 batch = 4800 	 loss = 1.69234 	 acc = 0.705 	 cost_time = 1036.786s
	 batch = 5200 	 loss = 1.86735 	 acc = 0.678 	 cost_time = 1122.887s
	 batch = 5600 	 loss = 1.94228 	 acc = 0.631 	 cost_time = 1208.570s
	 batch = 6000 	 loss = 1.69038 	 acc = 0.723 	 cost_time = 1294.581s
	 batch = 6400 	 loss = 2.03984 	 acc = 0.645 	 cost_time = 1381.299s
we get model ./checkpoints/bert20220614212958_rbtl1/bert_duie_word_epoch_0.bin
epoch = 1
*****For ee == 1, gg == 0:*****
*****masked_label: tensor([13778,   862, 13748,   791,   782,  2421,   128,  5307,   679, 13839,
          671, 19874, 15184,  4638,   510, 13830,   510,  7357,  6814, 16383,
        20327,  3221, 19178,   976,   671,  1059, 14969,  1469,  5314, 11378,
         3300, 16209,  4382, 17513, 12619,  8169,  8197,  4510,  3198,  1169,
         1062,  8392,  6627,  1962, 16408, 13735, 13811,  2769,  3683,  6844,
        14451, 20376, 16755, 18894, 13809,  1155, 17472,  4777,  1469,  1744,
         5783, 13748,  1355,  2190,  2408, 16321,  5384, 14129,  1343,  2845,
        13759,  1730,  1086,  5314,  2769,  1920, 14514,   172,  2769,   128,
          697, 16574, 15486,  3221,   517, 14120,  4638,  4294,  6820, 15167,
         7357,  3342, 15830,  5439, 14127,  3136, 13912,   749,   679,  8024,
        15619,  7566, 15250, 14519, 17269,   517,  2399,   741, 17489,  2025,
         5401,  2528, 15167, 14969,  5686,   704, 13784,  8516,  1155,   807,
         4636,  8594,  2399,  2769,  4706, 16858,  2769, 14972,  3300, 14442,
         2399,  7674, 15214,  2399,  3299,   677,  9459,  2399,  6564,   185,
        17332, 16229, 13769], device='cuda:0')*****
*****np.shape(masked_label): torch.Size([143])*****
*****masked_pre: tensor([19292,  3742, 14347,   671,  1039,  1920,   123,  5307,  4684, 16184,
          671, 18015, 20429,  4638,   510, 14126,   510,  7357,   671, 13876,
        20327,   676, 14456,   976,   671,   809, 14969,  8024,  1469, 11378,
         3300, 16209, 20852,  4868,   153,  8169,  8207,  4510,  3198,  1169,
         1062,  8118,  7350,  2769, 16408, 13735, 13811,  2769,  3683,  6844,
        18600, 20376, 16755, 13748, 13809,  4374, 17472,  4777,   510,  1744,
         5783, 15393,  1355,  2190,  2408, 15264,  3330, 14412,  2372,   671,
        13759,  1730,  6206,  1469,  2769,  1920, 14514,   172,  2769,  8108,
          704,  4638, 19439,  3221,   517, 13728,  4638,  4294,  6820, 15167,
         3342,  3342,   671,  5439, 15417,  3136, 13912,   679,   679,  8024,
        15167,  2110, 15214, 13839, 17442,   517,  2399,   741, 16229, 16229,
          122,  1453, 15167, 14969,   702,   704, 13784,  8447,  1155,   807,
         4636,  9166,  2399,  2769,  3801, 16858,  2769, 14400,  3300, 14442,
         2399,  7674, 15214,  2399,  3299,   677,  8138,  2399,  5722,   185,
        17332, 16229,   712], device='cuda:0')*****
*****np.shape(masked_pre): torch.Size([143])*****
	 batch = 400 	 loss = 1.94490 	 acc = 0.656 	 cost_time = 1492.152s
	 batch = 800 	 loss = 2.14125 	 acc = 0.586 	 cost_time = 1578.473s
	 batch = 1200 	 loss = 1.33632 	 acc = 0.750 	 cost_time = 1664.776s
	 batch = 1600 	 loss = 2.50845 	 acc = 0.565 	 cost_time = 1750.816s
	 batch = 2000 	 loss = 1.99025 	 acc = 0.664 	 cost_time = 1836.997s
	 batch = 2400 	 loss = 2.04966 	 acc = 0.616 	 cost_time = 1923.329s
	 batch = 2800 	 loss = 1.26994 	 acc = 0.736 	 cost_time = 2009.236s
	 batch = 3200 	 loss = 1.62492 	 acc = 0.742 	 cost_time = 2095.444s
	 batch = 3600 	 loss = 1.53079 	 acc = 0.739 	 cost_time = 2181.852s
	 batch = 4000 	 loss = 2.15260 	 acc = 0.639 	 cost_time = 2267.148s
	 batch = 4400 	 loss = 1.27598 	 acc = 0.771 	 cost_time = 2353.474s
	 batch = 4800 	 loss = 1.74879 	 acc = 0.657 	 cost_time = 2439.808s
	 batch = 5200 	 loss = 2.02268 	 acc = 0.673 	 cost_time = 2525.840s
	 batch = 5600 	 loss = 1.31553 	 acc = 0.735 	 cost_time = 2611.955s
	 batch = 6000 	 loss = 2.42692 	 acc = 0.602 	 cost_time = 2697.285s
	 batch = 6400 	 loss = 2.25586 	 acc = 0.626 	 cost_time = 2783.520s
we get model ./checkpoints/bert20220614212958_rbtl1/bert_duie_word_epoch_1.bin
epoch = 2
*****For ee == 2, gg == 0:*****
*****masked_label: tensor([15455, 13734,   517,  3837, 13740,  1400,  2207, 15030, 19489, 14595,
          518,  8024, 20120, 14595,  4638, 16266,  4959, 18662,  1158, 19191,
          754,  8197, 14985, 16209,  8024, 14244, 16919,  8024,  8024, 17470,
         5439,  2245, 17909, 16257,  4511,  3299, 13925, 14801,  7831,  8024,
         3851, 15413, 13746,  8024, 13746, 13809, 15050,  2857,  1199, 17552,
         2399,  1196,  1346,  2399,  3684,  1146,  3315, 15167, 15167,  3727,
        17746, 15312, 14801, 13866, 17730,  3472,  5468, 16386, 17825,  7339,
        16793,  2110,  3136, 14096,  8024,  1059,   704,  4960,  6668,  3221,
         3633,  1196,   518,  2418, 14273,  6963,  3221, 18369, 13809,  5165,
        15715, 15892,  6225, 13887,  4638,  4028,  8024,  5445,   679,  8024,
        13866, 15275], device='cuda:0')*****
*****np.shape(masked_label): torch.Size([102])*****
*****masked_pre: tensor([21044,   677,   517,  3837, 13740,   704,  1921, 15151, 19489, 14595,
          518,  8024, 15619, 14595,  4638, 14978,  4959, 15634,  1158, 19191,
          754,  8197, 17431, 16209,  8024, 20566, 20566,  8024,  8024, 17470,
         5439,  6624, 17909,  4403,  1957,  3299, 13925, 14801, 17956,  8024,
         3851, 15413, 13746,  8024, 13746, 13809, 15050,  1076,  1199, 17552,
         2399, 13784,  1346,  2399,  3684,  1146,   727, 15167, 15167,  3727,
        17746, 15312, 14801, 13866, 17730,  5074,  5468, 15393, 15393,  7339,
        16793,  2110,  3136, 17214,  8024,   704,   704,  5564,  6668,  3221,
         3633,  2767,   518,  2418, 14273,  6963,  3221, 18369, 13809,  5165,
        15715, 15892,  6225, 13887,  4638,  4028,  8024,  5445,   679,  8024,
        13866, 15275], device='cuda:0')*****
*****np.shape(masked_pre): torch.Size([102])*****
	 batch = 400 	 loss = 1.52741 	 acc = 0.690 	 cost_time = 2894.487s
	 batch = 800 	 loss = 1.76834 	 acc = 0.673 	 cost_time = 2980.812s
	 batch = 1200 	 loss = 1.53400 	 acc = 0.707 	 cost_time = 3067.360s
	 batch = 1600 	 loss = 2.15370 	 acc = 0.625 	 cost_time = 3153.572s
	 batch = 2000 	 loss = 2.02450 	 acc = 0.654 	 cost_time = 3239.777s
	 batch = 2400 	 loss = 1.22698 	 acc = 0.750 	 cost_time = 3325.904s
	 batch = 2800 	 loss = 1.20217 	 acc = 0.746 	 cost_time = 3412.189s
	 batch = 3200 	 loss = 1.22879 	 acc = 0.767 	 cost_time = 3498.087s
	 batch = 3600 	 loss = 1.72431 	 acc = 0.661 	 cost_time = 3584.207s
	 batch = 4000 	 loss = 1.61967 	 acc = 0.662 	 cost_time = 3670.570s
	 batch = 4400 	 loss = 1.31683 	 acc = 0.725 	 cost_time = 3756.783s
	 batch = 4800 	 loss = 1.24800 	 acc = 0.763 	 cost_time = 3842.890s
	 batch = 5200 	 loss = 1.37060 	 acc = 0.737 	 cost_time = 3928.873s
	 batch = 5600 	 loss = 1.58606 	 acc = 0.701 	 cost_time = 4014.623s
	 batch = 6000 	 loss = 1.21981 	 acc = 0.743 	 cost_time = 4101.157s
	 batch = 6400 	 loss = 1.37401 	 acc = 0.724 	 cost_time = 4187.219s
we get model ./checkpoints/bert20220614212958_rbtl1/bert_duie_word_epoch_2.bin
epoch = 3
	 batch = 400 	 loss = 1.48563 	 acc = 0.711 	 cost_time = 4297.845s
	 batch = 800 	 loss = 1.59237 	 acc = 0.695 	 cost_time = 4384.175s
	 batch = 1200 	 loss = 1.91465 	 acc = 0.689 	 cost_time = 4470.207s
	 batch = 1600 	 loss = 2.41223 	 acc = 0.653 	 cost_time = 4556.469s
	 batch = 2000 	 loss = 0.75915 	 acc = 0.815 	 cost_time = 4642.397s
	 batch = 2400 	 loss = 1.78448 	 acc = 0.628 	 cost_time = 4729.124s
	 batch = 2800 	 loss = 1.05889 	 acc = 0.798 	 cost_time = 4815.646s
	 batch = 3200 	 loss = 1.73727 	 acc = 0.680 	 cost_time = 4902.630s
	 batch = 3600 	 loss = 1.90839 	 acc = 0.672 	 cost_time = 4989.175s
	 batch = 4000 	 loss = 1.16612 	 acc = 0.735 	 cost_time = 5076.279s
	 batch = 4400 	 loss = 1.30484 	 acc = 0.772 	 cost_time = 5163.145s
	 batch = 4800 	 loss = 1.48040 	 acc = 0.748 	 cost_time = 5250.214s
	 batch = 5200 	 loss = 1.24807 	 acc = 0.737 	 cost_time = 5337.117s
	 batch = 5600 	 loss = 1.08346 	 acc = 0.752 	 cost_time = 5424.171s
	 batch = 6000 	 loss = 1.49368 	 acc = 0.721 	 cost_time = 5510.909s
	 batch = 6400 	 loss = 1.20030 	 acc = 0.718 	 cost_time = 5597.320s
we get model ./checkpoints/bert20220614212958_rbtl1/bert_duie_word_epoch_3.bin
epoch = 4
	 batch = 400 	 loss = 1.27793 	 acc = 0.744 	 cost_time = 5708.110s
	 batch = 800 	 loss = 1.92937 	 acc = 0.617 	 cost_time = 5794.899s
	 batch = 1200 	 loss = 0.93956 	 acc = 0.783 	 cost_time = 5881.714s
	 batch = 1600 	 loss = 2.00561 	 acc = 0.653 	 cost_time = 5968.747s
	 batch = 2000 	 loss = 1.18188 	 acc = 0.754 	 cost_time = 6055.578s
	 batch = 2400 	 loss = 1.14429 	 acc = 0.798 	 cost_time = 6142.387s
	 batch = 2800 	 loss = 1.61731 	 acc = 0.654 	 cost_time = 6228.826s
	 batch = 3200 	 loss = 1.30461 	 acc = 0.728 	 cost_time = 6316.330s
	 batch = 3600 	 loss = 1.54144 	 acc = 0.732 	 cost_time = 6403.534s
	 batch = 4000 	 loss = 1.71902 	 acc = 0.672 	 cost_time = 6490.475s
	 batch = 4400 	 loss = 1.36149 	 acc = 0.732 	 cost_time = 6577.208s
	 batch = 4800 	 loss = 1.22362 	 acc = 0.750 	 cost_time = 6664.002s
	 batch = 5200 	 loss = 1.23906 	 acc = 0.783 	 cost_time = 6751.099s
	 batch = 5600 	 loss = 1.73499 	 acc = 0.717 	 cost_time = 6837.858s
	 batch = 6000 	 loss = 1.62962 	 acc = 0.661 	 cost_time = 6924.610s
	 batch = 6400 	 loss = 1.25947 	 acc = 0.777 	 cost_time = 7011.595s
we get model ./checkpoints/bert20220614212958_rbtl1/bert_duie_word_epoch_4.bin
training done!
