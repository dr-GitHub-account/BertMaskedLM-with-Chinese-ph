*****Running model = BertForMaskedLM.from_pretrained(args.loadmodel)*****
The Initial Date = 6-15
bert is training which based on corpus ./train_data/duie_ner_train_1_spo_noWork_text.txt
The log information is saved in : ./log/bert/duie_ner_train_1_spo_noWork_text_20220615111533_rbtl1_train_log.txt
epoch = 0


*****For ee == 0, gg == 0:*****
*****inputs: {'input_ids': tensor([[  101,   756,  6937, 15312,  3180, 17009, 14334,  1905, 13811,   756,
          6937,  2255, 14136,  8024,  1777, 18919,  1762,  1406, 16505, 15312,
           103, 18056,  3777, 16807,  4638,  1931,   103,  1765, 15429,  8024,
          2600, 20538, 17973,  9083,  2398, 16232, 14119, 20084,  8024,  2124,
          1905, 14819,   707, 16807, 15413,   740, 15180, 14401,   680,  6817,
         14871, 15413,  4938,   103,   510,  3173, 18373,  4638,   769, 17575,
         14962,  8024,   103, 13811,  2255, 19262, 17746, 10606, 20013,   102,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0],
        [  101,  3342,   103,  1762,  5632, 15403,  4638,   741,   704,   103,
         19944,  8024,  6929,  3198, 14009,   517,  6205, 17009, 19438,   518,
          1157, 14214,  2864, 16086,  5310, 16395,  8024,  1063, 15264, 21034,
         18054,  5023,  2360, 15587,  1724, 13839,  2218,  4895, 15515,   749,
          1196,   103,  8024,  1343, 15575,  1166, 17695,  1765, 16232,  2937,
           103,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0],
        [  101,  1760,   103,   103, 16229, 15862,  6589,  1469,  2225, 18392,
         16229,  5468, 19987,  8024,  8715,  2399,   130,  3299,  4324, 18046,
          8024,  3221,  5739, 18525, 19987,   103, 14504, 14801,   103,  7481,
         17973, 11253,  2398, 16232, 14119, 20084,  8024,   782, 14423,   125,
           674,  8024,  7946, 13839,  1304,  8416,   103,  8024,   712, 19263,
          2134, 16193,   103,  1825, 17776, 16193,  8024,  5739, 19484,  3221,
           103, 16232, 20749, 19298,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0],
        [  101,   791, 16246,  8024,   753, 15859,  3173,   103,   517,  3221,
         13736, 16278,  4263, 15715,   518,  5303, 13811,  1762,   674, 13887,
          3309, 15578,   704,  7674, 16121,  8024,   868, 16346,   680,  5356,
         16346, 13839,  3221,  3341, 18689,   754,  7506, 14801,  4638,  4761,
         14456,  2391, 14457,  7509, 13784,   782,   152,   118,   162,  8332,
          8206,  1044, 17552,  8024,   800,  3295,   103,  1259, 15943,   103,
           510,  3330,   103, 15183,  1762, 14136,  4638,  1914, 13912,  4761,
         14456,  7506, 14801,  3625, 15854,   103,   683,   103,  3082, 14200,
          2958, 18723,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}*****
*****np.shape(inputs): (3,)*****
*****np.shape(inputs['input_ids']): torch.Size([4, 83])*****
*****np.shape(inputs['token_type_ids']): torch.Size([4, 83])*****
*****np.shape(inputs['attention_mask']): torch.Size([4, 83])*****
*****labels: tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          1298,  -100,  -100,  -100,  -100,  -100, 19292,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100, 15312,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  2247,  -100,  -100,  -100,  -100,  1298,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100],
        [ -100,  -100, 16872,  -100,  -100,  -100,  -100,  -100,  -100,  1091,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100, 18356,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          7032,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100],
        [ -100,  -100,  1046, 20084,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  2768,  -100,  -100,  8024,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,   110,  -100,  -100,  -100,
          -100,  -100,   711,  -100,  -100,  -100,  -100,  -100, 19484,  -100,
          2135,  -100, 19484,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100],
        [ -100,  -100,  -100,  -100,  -100,  -100,  -100, 16682,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  3341,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  3295,   711,  -100,  -100, 11873,
          -100,  -100, 17452,  -100,  -100,  -100,  -100,  -100,  -100,  -100,
          -100,  -100,  -100,  -100,  -100,  4638,  -100, 19839,  -100,  -100,
          -100,  -100,  -100]])*****
*****np.shape(labels): torch.Size([4, 83])*****
